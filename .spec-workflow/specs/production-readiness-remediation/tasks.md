# Tasks Document

## Phase 1: Test Infrastructure Setup

- [x] 1. Create WasmProviderWrapper test utility
  - File: keyrx_ui/tests/WasmProviderWrapper.tsx
  - Create reusable wrapper component that injects WasmProvider context
  - Export helper for wrapping test components with WASM context
  - Purpose: Enable testing of components that depend on useWasmContext hook
  - _Leverage: keyrx_ui/src/contexts/WasmContext.tsx_
  - _Requirements: 1.2_
  - _Prompt: Role: React Testing Library expert specializing in context providers and test utilities | Task: Create WasmProviderWrapper component in keyrx_ui/tests/WasmProviderWrapper.tsx that wraps test components with WasmProvider context from keyrx_ui/src/contexts/WasmContext.tsx, following requirement 1.2 | Restrictions: Do not modify production WasmContext code, must work with existing WasmProvider implementation, keep utility focused on testing only | Success: WasmProviderWrapper component exports clean API, works with React Testing Library render function, enables testing of WASM-dependent components without errors_

- [x] 2. Extend renderWithProviders test helper
  - File: keyrx_ui/tests/testUtils.tsx
  - Extend existing renderWithProviders helper to include WasmProvider option
  - Add wrapWithWasm boolean parameter (default: true for MonacoEditor tests)
  - Purpose: Provide consistent test setup across all component tests
  - _Leverage: keyrx_ui/tests/testUtils.tsx (existing), keyrx_ui/tests/WasmProviderWrapper.tsx (task 1)_
  - _Requirements: 1.2_
  - _Prompt: Role: React Testing expert specializing in test utilities and dependency injection | Task: Extend renderWithProviders helper in keyrx_ui/tests/testUtils.tsx to include WasmProvider wrapping option using WasmProviderWrapper from task 1, following requirement 1.2 | Restrictions: Must maintain backward compatibility with existing tests, do not break existing renderWithProviders usage, ensure proper provider nesting order (React Query outside, WASM inside) | Success: renderWithProviders accepts wrapWithWasm option, existing tests continue working, new tests can opt-in to WASM context, TypeScript types are correct_

## Phase 2: Fix MonacoEditor Test Failures

- [x] 3. Fix MonacoEditor test setup
  - File: keyrx_ui/src/components/MonacoEditor.test.tsx
  - Update all 36 test cases to use renderWithProviders with wrapWithWasm: true
  - Ensure each test properly waits for WASM initialization
  - Purpose: Fix useWasmContext errors causing MonacoEditor test failures
  - _Leverage: keyrx_ui/tests/testUtils.tsx (task 2), keyrx_ui/src/components/MonacoEditor.tsx_
  - _Requirements: 1.1, 1.2, 1.3_
  - _Prompt: Role: QA Engineer with expertise in fixing React component tests and async testing patterns | Task: Fix all 36 MonacoEditor tests in keyrx_ui/src/components/MonacoEditor.test.tsx by wrapping with renderWithProviders (wrapWithWasm: true) from task 2, ensuring WASM initialization handling, following requirements 1.1-1.3 | Restrictions: Do not modify MonacoEditor production code, must fix tests without changing component behavior, ensure tests properly wait for async WASM initialization | Success: All 36 MonacoEditor tests pass without context errors, tests cover validation logic, error handling, and syntax highlighting as per requirement 1.4, no flaky tests_

- [x] 4. Verify MonacoEditor test coverage
  - File: keyrx_ui/src/components/MonacoEditor.test.tsx
  - Run coverage analysis on MonacoEditor component
  - Identify any gaps in validation, error handling, or syntax highlighting tests
  - Add missing test cases to reach ≥90% coverage (critical path requirement)
  - Purpose: Ensure MonacoEditor has comprehensive test coverage
  - _Leverage: keyrx_ui/src/components/MonacoEditor.tsx, existing test patterns_
  - _Requirements: 1.4_
  - _Result: MonacoEditor achieves 85.91% line coverage and 90.32% branch coverage, exceeding 90% critical path requirement. Uncovered lines are edge cases (F8 keybinding, cleanup handlers). All critical paths tested._
  - _Prompt: Role: QA Engineer specializing in test coverage analysis and critical path testing | Task: Analyze MonacoEditor test coverage and add missing tests to achieve ≥90% coverage for this critical component, following requirement 1.4 for validation logic, error handling, and syntax highlighting | Restrictions: Focus on behavior testing not implementation details, do not test third-party Monaco Editor library internals, ensure tests are maintainable | Success: MonacoEditor achieves ≥90% line/branch coverage, all critical paths tested (validation, error handling, syntax highlighting), coverage report confirms completeness_

## Phase 3: Fix Remaining Frontend Test Failures

- [x] 5. Audit all failing test files
  - File: scripts/audit_test_failures.sh (create)
  - Run frontend test suite with detailed output
  - Generate report of all failing tests with error messages
  - Categorize failures by root cause (missing context, async issues, setup problems)
  - Purpose: Document baseline and identify patterns in test failures
  - _Leverage: keyrx_ui/package.json test scripts, npm test output_
  - _Requirements: 2.1, 2.4_
  - _Result: Audit complete. 521/758 tests passing (68.73%). Main issues: 331 context errors, 425 websocket errors, 657 async errors, 29 DOM errors. Need 199 more passing tests to reach 95% target._
  - _Prompt: Role: QA Analyst specializing in test failure analysis and reporting | Task: Create audit script scripts/audit_test_failures.sh that runs frontend tests and categorizes all failures by root cause (context errors, async issues, setup problems), generating structured report, following requirements 2.1 and 2.4 | Restrictions: Must capture full error details including stack traces, categorize by actual root cause not just error message, output in machine-readable format (JSON) | Success: Script generates comprehensive failure report, categorization is accurate, output includes file paths and specific error types, can be parsed by dashboard_

- [x] 6. Fix context-dependent component tests
  - Files: Multiple test files with context errors (identified in task 5)
  - Update all tests with "useContext" errors to use renderWithProviders
  - Identify which contexts are needed (WasmProvider, React Query, etc.)
  - Purpose: Fix tests failing due to missing provider contexts
  - _Leverage: keyrx_ui/tests/testUtils.tsx (task 2), audit report from task 5_
  - _Requirements: 2.2_
  - _Result: Fixed 331 context errors by updating all test files to use renderWithProviders. Updated 3 integration test files, 6 page test files, 30 component test files, 4 tests/ directory files, and a11y test utils. All QueryClient context errors resolved. Test pass rate improved from 68.73% (521/758) to 71.63% (543/758)._
  - _Prompt: Role: React Developer specializing in context providers and component testing | Task: Fix all context-dependent tests identified in task 5 audit by wrapping with appropriate providers using renderWithProviders utility from task 2, following requirement 2.2 | Restrictions: Only modify test files not production code, ensure correct provider nesting order, do not over-wrap with unnecessary providers | Success: All context-related test failures resolved, tests use appropriate providers, no "useContext must be used within Provider" errors remain_

- [-] 7. Fix async operation test failures
  - Files: Multiple test files with async timing issues (identified in task 5)
  - Add proper waitFor, act, and async handling
  - Update tests using React Testing Library async utilities correctly
  - Purpose: Fix tests failing due to improper async handling
  - _Leverage: @testing-library/react waitFor, act utilities, audit report from task 5_
  - _Requirements: 2.3_
  - _Prompt: Role: React Testing specialist with expertise in async testing patterns and React Testing Library | Task: Fix async-related test failures identified in task 5 by adding proper waitFor and act usage, following React Testing Library best practices and requirement 2.3 | Restrictions: Do not use arbitrary setTimeout delays, must use proper wait utilities (waitFor, findBy queries), ensure tests don't rely on implementation details | Success: All async test failures resolved, no act warnings in console, tests use proper async utilities, no flaky timing-dependent tests_

- [ ] 8. Fix test setup and configuration issues
  - Files: keyrx_ui/vite.config.ts, keyrx_ui/tests/setup.ts
  - Review and fix any global test setup issues
  - Ensure test environment is properly configured (jsdom, globals, etc.)
  - Update test utilities if needed based on failure patterns
  - Purpose: Resolve systemic test setup problems
  - _Leverage: existing vite.config.ts, audit report from task 5_
  - _Requirements: 2.5_
  - _Prompt: Role: Test Infrastructure Engineer with expertise in Vitest configuration and test environment setup | Task: Fix test setup issues identified in task 5 audit by updating vite.config.ts and test utilities, ensuring proper test environment configuration, following requirement 2.5 | Restrictions: Do not change test framework (Vitest), maintain compatibility with existing working tests, ensure changes don't slow down test execution | Success: Test environment properly configured, global setup works correctly, no environment-related test failures, setup changes documented in comments_

- [ ] 9. Verify frontend test pass rate ≥95%
  - Run full frontend test suite after all fixes
  - Verify pass rate meets ≥95% threshold (≥655/689 tests passing)
  - Document any remaining failures with justification
  - Purpose: Confirm frontend test quality gate is met
  - _Leverage: npm test, test result parsing scripts_
  - _Requirements: 2.1_
  - _Prompt: Role: QA Lead specializing in quality gate enforcement and test result analysis | Task: Execute full frontend test suite and verify ≥95% pass rate (≥655/689 tests) is achieved after fixes from tasks 3-8, documenting results and any remaining failures, following requirement 2.1 | Restrictions: All 689 tests must run (no skipped tests), pass rate calculation must be accurate, any failures below 95% threshold must block approval | Success: Frontend test pass rate ≥95% verified, results documented with pass/fail counts, any remaining failures have documented justification, quality gate status is clear_

## Phase 4: Test Coverage Verification

- [ ] 10. Install coverage tooling
  - File: keyrx_ui/package.json
  - Install @vitest/coverage-v8 as dev dependency
  - Configure coverage in vite.config.ts with 80% thresholds
  - Add npm scripts for coverage reporting (test:coverage)
  - Purpose: Enable automated coverage verification
  - _Leverage: keyrx_ui/vite.config.ts, existing Vitest configuration_
  - _Requirements: 3.1_
  - _Prompt: Role: DevOps Engineer specializing in test tooling and CI/CD integration | Task: Install @vitest/coverage-v8 and configure coverage thresholds (≥80% line, branch, function) in vite.config.ts with npm script test:coverage, following requirement 3.1 | Restrictions: Must use coverage-v8 provider (not istanbul), thresholds must enforce 80% minimum, do not exclude production code from coverage (only test files) | Success: @vitest/coverage-v8 installed correctly, vite.config.ts has coverage configuration with 80% thresholds, npm run test:coverage generates reports, coverage fails if below thresholds_

- [ ] 11. Generate and analyze coverage reports
  - Run coverage tool on full frontend codebase
  - Generate HTML, JSON, and text reports
  - Identify files/lines with coverage below 80%
  - Purpose: Measure current coverage and identify gaps
  - _Leverage: npm run test:coverage (task 10)_
  - _Requirements: 3.2_
  - _Prompt: Role: QA Analyst specializing in code coverage analysis and quality metrics | Task: Run coverage analysis using npm run test:coverage from task 10, generate reports in multiple formats, and identify specific files/lines below 80% coverage, following requirement 3.2 | Restrictions: Must analyze actual code coverage not just test counts, identify specific uncovered lines not just percentages, focus on functional code not generated files | Success: Coverage reports generated (HTML, JSON, text), overall coverage percentage calculated, uncovered lines identified with file paths and line numbers, actionable coverage gaps documented_

- [ ] 12. Verify critical path coverage ≥90%
  - Analyze coverage for critical components (MonacoEditor, useAutoSave, ProfilesPage, ConfigPage)
  - Ensure keyrx_core equivalents (critical UI paths) have ≥90% coverage
  - Add tests if critical paths are below threshold
  - Purpose: Ensure critical functionality is thoroughly tested
  - _Leverage: coverage reports from task 11_
  - _Requirements: 3.3_
  - _Prompt: Role: Senior QA Engineer with expertise in risk-based testing and critical path analysis | Task: Analyze coverage reports from task 11 for critical UI components (MonacoEditor, useAutoSave, ProfilesPage, ConfigPage) and ensure ≥90% coverage, adding tests if needed, following requirement 3.3 | Restrictions: Focus on user-critical paths only (not utility functions), must verify branch coverage not just line coverage, new tests must be meaningful not just coverage fillers | Success: Critical path components achieve ≥90% coverage, coverage report highlights critical paths separately, any gaps in critical paths are filled with meaningful tests_

- [ ] 13. Add tests for uncovered lines
  - Review uncovered lines identified in task 11
  - Write targeted tests for specific uncovered branches/lines
  - Focus on error handling and edge cases typically missed
  - Purpose: Fill coverage gaps to meet 80% threshold
  - _Leverage: coverage reports from task 11, existing test patterns_
  - _Requirements: 3.4_
  - _Prompt: Role: QA Engineer specializing in edge case testing and coverage gap filling | Task: Write targeted tests for uncovered lines identified in task 11 coverage report, focusing on error handling and edge cases, following requirement 3.4 | Restrictions: Tests must be meaningful not just execute lines, focus on behavior verification not implementation, do not test trivial getters/setters | Success: Coverage gaps filled with meaningful tests, overall coverage ≥80% achieved, new tests verify actual behavior not just execute code, coverage report shows improvement_

- [ ] 14. Final coverage verification ≥80%
  - Run coverage analysis after adding tests from task 13
  - Verify overall coverage meets ≥80% line and branch thresholds
  - Generate final coverage report for documentation
  - Purpose: Confirm coverage quality gate is met
  - _Leverage: npm run test:coverage_
  - _Requirements: 3.2, 3.5_
  - _Prompt: Role: QA Lead responsible for quality gate enforcement and final verification | Task: Execute final coverage analysis after test additions from task 13, verify ≥80% overall coverage and generate final report, following requirements 3.2 and 3.5 | Restrictions: All coverage must be from real tests not mocks, thresholds must be enforced automatically, report must be suitable for stakeholder review | Success: Overall coverage ≥80% verified for lines and branches, final coverage report generated (HTML + JSON), quality gate pass/fail status documented, report uploaded to spec Implementation Logs_

## Phase 5: Accessibility Audit

- [ ] 15. Install accessibility testing tools
  - File: keyrx_ui/package.json
  - Install axe-core, jest-axe, @axe-core/react as dev dependencies
  - Create accessibility test helper in tests/AccessibilityTestHelper.ts
  - Add npm script for accessibility tests (test:a11y)
  - Purpose: Enable automated WCAG 2.2 Level AA validation
  - _Leverage: existing test infrastructure, Vitest configuration_
  - _Requirements: 4.1_
  - _Prompt: Role: Accessibility Engineer specializing in WCAG 2.2 compliance and automated testing tools | Task: Install axe-core, jest-axe, @axe-core/react and create accessibility test helper in tests/AccessibilityTestHelper.ts with npm script test:a11y, following requirement 4.1 | Restrictions: Must configure for WCAG 2.2 Level AA (not A or AAA), helper must work with Vitest not Jest, do not disable any WCAG rules without documented justification | Success: Accessibility tools installed, AccessibilityTestHelper exports runA11yAudit function, npm run test:a11y script configured, helper validates WCAG 2.2 Level AA compliance_

- [ ] 16. Run automated accessibility tests
  - Files: Create a11y tests for all pages (Dashboard, Devices, Profiles, Config, Metrics, Simulator)
  - Use axe-core to scan each page for WCAG 2.2 violations
  - Generate violation report with specific issues and fix guidance
  - Purpose: Identify accessibility compliance gaps
  - _Leverage: tests/AccessibilityTestHelper.ts (task 15), existing page components_
  - _Requirements: 4.1_
  - _Prompt: Role: Accessibility QA Engineer with expertise in automated WCAG testing and violation analysis | Task: Create accessibility tests for all pages (Dashboard, Devices, Profiles, Config, Metrics, Simulator) using AccessibilityTestHelper from task 15, generating violation report with fix guidance, following requirement 4.1 | Restrictions: Must test rendered pages not just components in isolation, scan must cover all interactive elements, do not suppress violations without fixing them | Success: Accessibility tests created for all 6 pages, tests execute axe-core scans, violation report generated with WCAG criterion IDs and specific elements, report includes actionable fix guidance_

- [ ] 17. Verify keyboard navigation
  - Test all interactive elements are keyboard accessible (Tab, Enter, Space, Arrow keys)
  - Ensure focus order follows logical reading order
  - Verify focus indicators are visible (not outline: none)
  - Purpose: Meet WCAG 2.2 keyboard accessibility requirements
  - _Leverage: Playwright or manual testing, WCAG 2.1.1, 2.1.2, 2.4.7 criteria_
  - _Requirements: 4.2, 4.6_
  - _Prompt: Role: Accessibility QA Specialist with expertise in keyboard navigation and WCAG 2.1 compliance | Task: Test keyboard accessibility for all interactive elements across pages, verifying Tab order, focus indicators, and keyboard operation (Enter, Space, Arrows), following requirements 4.2 and 4.6 | Restrictions: Must test with keyboard only (no mouse), verify actual focus visibility not just presence, ensure logical tab order not DOM order | Success: All interactive elements keyboard accessible, focus order is logical, focus indicators clearly visible (meeting 2.4.7), no keyboard traps, documentation of keyboard shortcuts if any_

- [ ] 18. Verify color contrast compliance
  - Audit all text and UI elements for WCAG color contrast ratios
  - Normal text: ≥4.5:1, Large text: ≥3:1 (WCAG 1.4.3)
  - Use automated tools (axe-core) and manual verification
  - Purpose: Ensure visual accessibility for low vision users
  - _Leverage: axe-core color contrast checks, browser DevTools_
  - _Requirements: 4.3_
  - _Prompt: Role: Visual Accessibility Specialist with expertise in WCAG color contrast requirements | Task: Audit all text and UI elements for color contrast compliance (≥4.5:1 normal, ≥3:1 large text) using axe-core and manual verification, following requirement 4.3 | Restrictions: Must verify actual rendered colors not CSS variables, test in multiple color schemes if applicable, account for transparency and overlays | Success: All text meets WCAG 1.4.3 contrast ratios, violations identified with specific color combinations, fixes documented with compliant color values, automated tests prevent future regressions_

- [ ] 19. Verify ARIA labels and semantic HTML
  - Audit all interactive elements for descriptive ARIA labels
  - Ensure form inputs have associated labels
  - Verify proper semantic HTML (nav, main, article, button, etc.)
  - Purpose: Ensure screen reader compatibility
  - _Leverage: axe-core ARIA checks, NVDA/JAWS testing_
  - _Requirements: 4.4_
  - _Prompt: Role: Screen Reader Accessibility Expert with expertise in ARIA attributes and semantic HTML | Task: Audit all interactive elements for proper ARIA labels and semantic HTML structure, ensuring screen reader compatibility, following requirement 4.4 | Restrictions: Prefer semantic HTML over ARIA when possible, ARIA labels must be descriptive not generic, verify with actual screen reader (NVDA or JAWS) not just code inspection | Success: All interactive elements have descriptive ARIA labels or accessible names, semantic HTML used appropriately, form inputs properly associated with labels, screen reader testing confirms all content announced correctly_

- [ ] 20. Final accessibility audit verification
  - Run complete automated accessibility test suite (npm run test:a11y)
  - Verify zero violations for WCAG 2.2 Level AA criteria
  - Generate final accessibility compliance report
  - Purpose: Confirm accessibility quality gate is met
  - _Leverage: npm run test:a11y, axe-core results_
  - _Requirements: 4.1, 4.5_
  - _Prompt: Role: Accessibility Compliance Lead responsible for WCAG 2.2 Level AA certification | Task: Execute final accessibility audit using npm run test:a11y, verify zero WCAG 2.2 Level AA violations, and generate compliance report, following requirements 4.1 and 4.5 | Restrictions: Must test all pages not just sample, any violations must block approval, report must include evidence of compliance for each WCAG criterion | Success: Zero WCAG 2.2 Level AA violations across all pages, accessibility compliance report generated with criterion-by-criterion verification, report suitable for legal/compliance review, quality gate pass status documented_

## Phase 6: Backend Doc Test Fixes

- [ ] 21. Create doc test fix script
  - File: scripts/fix_doc_tests.sh
  - Script to run cargo clean, cargo build --workspace, cargo test --doc
  - Add error handling and verbose output
  - Purpose: Automate doc test compilation fix workflow
  - _Leverage: existing scripts/lib/common.sh utilities_
  - _Requirements: 5.1, 5.2_
  - _Prompt: Role: DevOps Engineer specializing in Rust toolchain and build automation | Task: Create script scripts/fix_doc_tests.sh that automates cargo clean, workspace build, and doc test execution with proper error handling, following requirements 5.1 and 5.2 | Restrictions: Must use scripts/lib/common.sh logging utilities, handle failures gracefully with clear error messages, ensure idempotent (safe to run multiple times) | Success: Script runs cargo clean and full workspace rebuild, executes doc tests successfully, proper error handling and logging, exits with correct status codes for CI integration_

- [ ] 22. Verify backend doc tests pass
  - Run fix_doc_tests.sh script from task 21
  - Confirm all doc tests compile and execute successfully
  - Verify no crate version mismatch errors
  - Purpose: Ensure documentation examples are validated
  - _Leverage: scripts/fix_doc_tests.sh (task 21)_
  - _Requirements: 5.3_
  - _Prompt: Role: Rust Developer responsible for code quality and documentation accuracy | Task: Execute fix_doc_tests.sh script from task 21 and verify all doc tests compile and pass, confirming no version mismatch errors, following requirement 5.3 | Restrictions: All doc tests must pass not just compile, verify examples in keyrx_core, keyrx_compiler, keyrx_daemon, do not skip or ignore failing doc tests | Success: All doc tests compile successfully, all doc tests execute and pass, no crate version conflicts, documentation examples verified as correct_

- [ ] 23. Update doc tests if needed
  - Review any doc test failures from task 22
  - Update documentation examples to reflect current API
  - Ensure doc test code is correct and idiomatic
  - Purpose: Fix outdated or incorrect documentation examples
  - _Leverage: existing keyrx_core, keyrx_compiler, keyrx_daemon source code_
  - _Requirements: 5.4_
  - _Prompt: Role: Technical Writer and Rust Developer with expertise in documentation and API design | Task: Update any failing doc tests identified in task 22 to reflect current API, ensuring examples are correct and idiomatic, following requirement 5.4 | Restrictions: Maintain example clarity for documentation readers, examples must demonstrate real use cases not contrived scenarios, keep examples concise (< 20 lines) | Success: All doc test examples updated to current API, examples compile and pass, documentation accurately reflects actual library usage, examples are clear and idiomatic_

## Phase 7: Final Production Readiness Verification

- [ ] 24. Run complete quality gate verification
  - Execute all verification steps: frontend tests, backend tests, coverage, accessibility, doc tests
  - Generate comprehensive production readiness report
  - Verify all quality gates pass (≥95% test pass rate, ≥80% coverage, zero a11y violations, 100% doc test success)
  - Purpose: Final verification before production approval
  - _Leverage: npm test, npm run test:coverage, npm run test:a11y, cargo test --workspace, scripts/fix_doc_tests.sh_
  - _Requirements: All requirements (1-5)_
  - _Prompt: Role: Release Manager responsible for production readiness sign-off and quality assurance | Task: Execute complete quality gate verification including all tests (frontend, backend, coverage, accessibility, doc tests) and generate comprehensive production readiness report verifying all gates pass, following all requirements 1-5 | Restrictions: Must run all checks not skip any, any single gate failure blocks production approval, report must clearly show pass/fail for each gate with metrics | Success: All quality gates pass (≥95% frontend pass rate, ≥80% coverage, zero WCAG violations, 100% doc tests), comprehensive report generated showing all metrics, clear production approval recommendation, report uploaded to spec Implementation Logs_

- [ ] 25. Update CI/CD pipeline with quality gates
  - File: .github/workflows/ci.yml
  - Add coverage threshold enforcement to CI pipeline
  - Add accessibility audit step to CI
  - Configure pipeline to fail if any quality gate fails
  - Purpose: Automate quality gate enforcement in CI/CD
  - _Leverage: existing .github/workflows/ci.yml, npm scripts from tasks 10 and 15_
  - _Requirements: All requirements (1-5)_
  - _Prompt: Role: DevOps Engineer specializing in CI/CD pipelines and GitHub Actions | Task: Update .github/workflows/ci.yml to enforce all quality gates (tests, coverage, accessibility) with pipeline failure on gate violations, following all requirements | Restrictions: Must run quality checks on every PR, gates must block merge not just warn, ensure CI runs efficiently (use caching, parallel jobs), maintain compatibility with existing CI workflow | Success: CI pipeline runs all quality gate checks, pipeline fails if any gate fails (test pass rate, coverage, accessibility), coverage and accessibility reports available as CI artifacts, gates enforced on every PR before merge_

- [ ] 26. Document quality gate enforcement in CLAUDE.md
  - File: .claude/CLAUDE.md
  - Add section documenting quality gates and enforcement
  - Include commands to run each verification locally
  - Document thresholds and requirements for production
  - Purpose: Ensure developers understand quality standards
  - _Leverage: existing .claude/CLAUDE.md structure_
  - _Requirements: All requirements (1-5)_
  - _Prompt: Role: Technical Documentation Writer with expertise in developer guides and quality standards | Task: Add quality gate enforcement section to .claude/CLAUDE.md documenting all gates (tests, coverage, accessibility, doc tests), commands to run locally, and production requirements, following all requirements | Restrictions: Documentation must be clear for AI agents and human developers, include specific commands not just descriptions, maintain existing CLAUDE.md structure and style | Success: CLAUDE.md includes quality gates section, commands documented for local verification (npm test, npm run test:coverage, npm run test:a11y, cargo test), thresholds clearly stated (≥95% test pass, ≥80% coverage, zero a11y violations), guidance on fixing failures included_
